# GRU Model Implementation from Scratch

This repository contains a pure Python implementation of the Gated Recurrent Unit (GRU) model from scratch. It serves as an educational resource to understand the inner workings of GRU models without relying on high-level libraries such as TensorFlow or PyTorch.

## Introduction

The Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) designed to address the vanishing gradient problem and improve the learning of long-term dependencies. This repository showcases the step-by-step construction of a GRU model using Python and PySpark.

In addition to the implementation, the repository provides comparisons between different GRU configurations, analyzing their performance under various conditions. By exploring these configurations, users can gain insights into the impact of different hyperparameters on the model's effectiveness.

This project is ideal for students, researchers, and anyone interested in deep learning and neural networks who wants to deepen their understanding of GRUs by building and experimenting with them from the ground up.

The results of these comparisons help illustrate how different settings influence the model's learning ability and performance. 

